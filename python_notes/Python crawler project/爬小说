import requests
from bs4 import BeautifulSoup
import lxml
import chardet
import time
import random
import os
import json
from fake_useragent import UserAgent
from tqdm import tqdm
import string
"""
#初始化配置
class Config:
    BASE_URL = "https://www.22biqu.com/bique1346"
    START_ID =12278419
    END_ID = 12278430 
    OUTPUT_DIR = "许仙志"
    PROGRESS_FILE = os.path.join(OUTPUT_DIR,"progress.json")
    MIN_DELAY = 1
    MAX_DELAY = 3
    RETRY_TIMES = 3



#创建输出目录
os.makedirs(Config.OUTPUR_DIR,exist_ok=True)

class NovelSpider:
    def __init__(self):
        self.ua =UserAgent()
        self.session = requests.Session()
        self.load_progress()
    
    def get_random_headers(self):
        return{
            'User-Agent':self.ua.random,
            'Accept':'text/html,appliction/xhtml+xml,application/xml;q=0.9,*/*;q==0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': f'{Config.BASE_URL}/',
            'DNT': '1',  # 禁止追踪
        }
    
    def load_progress(self):
        try:
            with open(Config.PROGRESS_FILE,'r') as f:
                self.progress = json.load(f)
            print(f"加载进度：已下载{len(self.progress['completed'])}"章)

        except:
            self.progress = {
                'last_id': Config.START_ID - 1,
                'completed': [],
                'failed': []
            }
    
    def save_progress(self):
        with open(Config.PROGRESS_FILE,'w') as f:
            json.dump(self.progress,f,indent=2)
    
    def get_page(self,url):
        for _ in range(Config.RETRY_TIMES):
            try:
                resonse = self.session.get(
                    url,
                    headers=self.get_random_headers(),
                    timeout=10,
                    proxies={'http':'socks5;//127.0.0.1:1080','https':'socks;//127.0.0.1:1080'}#示例处理
                )
                response.raise_for_status()

                #自动检测编码
                if 'charset' not in response.headers.get('content-type',''):
                    encoding = chardet.detect(response.content)['encoding']
                    response.encoding = encoding if encoding else 'utf-8'

                return response
                
            except Exception as e:
                print(f"请求失败：{e}")
                time.sleep(random.uniform(2,5))
        return None

    def process_chapter(self,chapter_id):
        if chapter_id in self.progress['completed']:
            return True

        url = f"{Config.BASE_URL}/{chapter_id}.html"
        print(f"正在处理：{url}")

        response = self.get_page(url)
        if not response:
            self.progress['failed'].append(chapter_id)
            return False
        soup = BeautifulSoup(response.text,'lxml')

        #获取章节标题
        title = soup.find('h1')
        chapter_title = title.get_text().strip() if title else f"第{chapter_id - Config.START_ID+1}章"


        #获取正文
        content =soup.find("div",id='content')
        if not content:
            self.progress['failed'].append(chapter_id)
                return False
        
        #清楚内容
        text = '\n'.join(line.strip()
                        for line in content.get.text().split('\n')
                        if line.strip())
                    
        #保存章节
        filename = f"{chapter_id}_{chapter_title}.txt".replace('/','_')
        with open(os.path.join(Config.OUTPUT_DIR,filename)),'w',encoding='utf-8') as f
        f.write(text)

        #更新进度
        self.progress['completed'].append(chapter_id)
        self.progress['last_id']=chapter_id
        self.save_progress()

        return True
    
    def run(self):
        try:
            for chapter_id in range(self.progtress['last_id']+1,Config.END_ID + 1):
                self.process_chapter(chapter_id)
                delay = random.uniform(Config.MIN_DELAY,Config.MAX_DELAY)
                time.sleep(delay)
        
        except KeyboardInterrupt:
            print("\中断，正在保存进度")
        finally:
            self.save_progress()
            print(f"完成，合格：{len(self.progress['completed'])},失败：{len(self.progress['failed'])}")

        if __name__ =='__mian__':
            spider = NovelSpider()
            spider.run()
            

"""
"""
    https://www.22biqu.com/biqu13446/12278420.html
    https://www.22biqu.com/biqu13446/14723358.html
"""
#为了连续爬取，我要对网址进行递增

"""#输入爬取网址
url="https://www.22biqu.com/biqu13446/12278419.html"

"""




# 初始化随机用户代理生成器

headers = {
    'User-Agent': UserAgent().random,  # 每次请求生成随机UA
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Referer': 'https://www.22biqu.com/',
    'DNT': '1'  # 禁止追踪
}

#基准目录
base_directory = r'D:\games\computer-learning\爬取的小说'
if not os.path.exists(base_directory):
    os.makedirs(base_directory)


#目标目录
target_directory=os.path.join(base_directory,'许仙志')
if not os.path.exists(target_directory):
    os.makedirs(target_directory)
    print(f"{target_directory}已创建")
else:
    print("已存在")
min_interval=1
max_interval=5

#可视化处理
for i in tqdm(range(12278419, 12278429), 
             desc="正在爬取小说", 
             unit="章",
             ncols=100,
             dynamic_ncols=True):

    #测试异常
    try:
        url=f"https://www.22biqu.com/biqu13446/{i}.html"
        url2=f"https://www.22biqu.com/biqu13446/{i}_2.html"

        #请求网站，获取原始内容
        response=requests.get(url,headers=headers)
        

        response.raise_for_status()
        
        #自动检察编码
        det= chardet.detect(response.content)
        actual_encoding=det['encoding']
        print(f"编码：{actual_encoding}")

        #使用正确编码解码
        response.encoding = actual_encoding
        html_content = response.text

        #解析内容
        soup=BeautifulSoup(html_content,'lxml')


        
        #确定好随机访问时间间隔
        interval= random.uniform(min_interval,max_interval)

        #获取章节标题
        title_tag = soup.find("h1",class_='title')
        raw_title = title_tag.get_text().strip() #if title_tag else f"第{i-12278418}章"
        print(raw_title)
        cleaned_title =raw_title
        """     clean_filename=raw_title
        # 加强版文件名清理
        def clean_filename(title):
            # 替换常见非法字符
            title = title.replace('\n', '').replace('\r', '').strip()
            # 只保留允许的字符
            valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
            cleaned = ''.join(c for c in title if c in valid_chars)
            # 确保不为空
            return cleaned or f"第{i-12278418}章"
        """
        #是否有下一页
        response_2=requests.get(url2,headers=headers)
        # 确保目录存在
        os.makedirs(target_directory, exist_ok=True)

        #获取正文内容
        content_div=soup.find("div",id='content')
        
        if content_div:
            #清理文本内容
            paragraphs = content_div.find_all('p')
            # 优化后的处理逻辑
        
            # 创建空列表存储处理后的段落
            processed_paragraphs = []

            #  遍历所有段落
            for para in paragraphs:
                # 移除段落首尾的空白字符（但保留段内格式）
                stripped_para = para.get_text().strip()

                # 跳过空段落（避免处理空白行）
                if not stripped_para:
                    continue
                
                # 为每个非空段落添加首行缩进
                
                indented_para = '　　' + stripped_para
                
                processed_paragraphs.append(indented_para)

            # 用单换行符合并所有段落（符合中文排版规范）
            cleaned_content = '\n'.join(processed_paragraphs)

            #保存为单独文件
            filename = os.path.join(target_directory, f"{cleaned_title}.txt")
            with open(filename,encoding='utf-8',mode='w') as f:
                f.write(cleaned_content)
            
            if  response_2:
                soup_2= BeautifulSoup(response_2.text,'lxml')
                houxu=soup_2.find("div",id='content')

                if houxu:
                    #找到<P>标签
                    paragraphs2 = houxu.find_all('p')
                    
                
                    # 创建空列表存储处理后的段落
                    processed_paragraphs2 = []

                    # 遍历所有段落
                    for para2 in paragraphs2:
                        # 移除段落首尾的空白字符（但保留段内格式）
                        stripped_para2 = para2.get_text().strip()

                        # 跳过空段落（避免处理空白行）
                        if not stripped_para2:
                            continue
                        
                        # 为每个非空段落添加首行缩进
                        
                        indented_para2 = '　　' + stripped_para2
                        
                        processed_paragraphs2.append(indented_para2)

                    #  用单换行符合并所有段落（符合中文排版规范）
                    houxu = '\n'.join(processed_paragraphs2)


                with open(filename,encoding='utf-8',mode='a') as f:
                    f.write(houxu)   
            else:         
                print(f"小说{cleaned_title}保存完成")
        else:
            print("未完成")
        
        time.sleep(interval)
    except Exception as e:
        tqdm.write((f"处理{url}发生错误：{e}"))
        continue
        