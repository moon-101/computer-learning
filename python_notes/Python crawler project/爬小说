import requests
from bs4 import BeautifulSoup
import lxml
import chardet
import time
import random
import os
import json
from fake_useragent import UserAgent

#初始化配置
class Config:
    BASE_URL = "https://www.22biqu.com/bique1346"
    START_ID =12278419
    END_ID = 12278430 
    OUTPUT_DIR = "许仙志"
    PROGRESS_FILE = os.path.join(OUTPUT_DIR,"progress.json")
    MIN_DELAY = 1
    MAX_DELAY = 3
    RETRY_TIMES = 3



#创建输出目录
os.makedirs(Config.OUTPUR_DIR,exist_ok=True)

class NovelSpider:
    def __init__(self):
        self.ua =UserAgent()
        self.session = requests.Session()
        self.load_progress()
    
    def get_random_headers(self):
        return{
            'User-Agent':self.ua.random,
            'Accept':'text/html,appliction/xhtml+xml,application/xml;q=0.9,*/*;q==0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': f'{Config.BASE_URL}/',
            'DNT': '1',  # 禁止追踪
        }
    
    def load_progress(self):
        try:
            with open(Config.PROGRESS_FILE,'r') as f:
                self.progress = json.load(f)
            print(f"加载进度：已下载{len(self.progress['completed'])}"章)

        except:
            self.progress = {
                'last_id': Config.START_ID - 1,
                'completed': [],
                'failed': []
            }
    
    def save_progress(self):
        with open(Config.PROGRESS_FILE,'w') as f:
            json.dump(self.progress,f,indent=2)
    
    def get_page(self,url):
        for _ in range(Config.RETRY_TIMES):
            try:
                resonse = self.session.get(
                    url,
                    headers=self.get_random_headers(),
                    timeout=10,
                    
                )



"""
    https://www.22biqu.com/biqu13446/12278420.html
    https://www.22biqu.com/biqu13446/14723358.html
"""
#为了连续爬取，我要对网址进行递增
#输入爬取网址
url="https://www.22biqu.com/biqu13446/12278419.html"
#模拟网络请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
for i in range(12278419,14723359):
    url=f"https://www.22biqu.com/biqu13446/{i}.html"

    #测试异常
    try:
        #请求网站，获取原始内容
        response=requests.get(url,headers=headers)
        response.raise_for_status()
        
        #自动检察编码
        det= chardet.detect(response.content)
        actual_encoding=det['encoding']
        print(f"编码：{actual_encoding}")

        #使用正确编码解码
        response.encoding = actual_encoding
        html_content = response.text

        #解析内容
        soup=BeautifulSoup(html_content,'lxml')

        #获取章节标题
        title_tag = soup.find('h1')
        chapter_title = title_tag.get_text().strip() if title_tag else f"第{i-12278418}章"
        
        #获取正文内容
        content_div=soup.find("div",id='content')
        if content_div:
            #清理文本内容
            text_content = content_div.get_text()
            cleaned_content ='\n'.join(line.strip() for line in text_content.split('\n') if line.strip())

            #保存为单独文件
            filename=f"许仙志/{chapter_title}.txt"
            with open(filename,encoding='utf-8') as f:
                f.write(cleaned_content)
            
            print("小说保存完成")
        else:
            print("未完成")
    except Exception as e:
        print(f"处理{url}发生错误：{e}")



